
ShelfScan Demo - Product Recognition App Documentation
This is an iOS app that helps users locate products in a store using computer vision. The app combines product search capabilities with real-time object detection to identify and highlight products through the camera feed.
Overall Architecture
The app follows an MVVM pattern with clear separation between UI, business logic, and data layers. It's built using SwiftUI for the interface and integrates TensorFlow Lite models for computer vision tasks.
Core Flow

Authentication - Users log in with credentials and specify a store number
Product Search - Users search for products by name, UPC, or category
Product Selection - Users pick a product they want to find in the store
Real-time Detection - Camera feed processes frames to detect and highlight the selected product

Authentication System
The authentication layer handles secure token management and user verification.
KeychainService.swift provides secure storage for authentication tokens using the iOS Keychain. It's implemented as a singleton with generic methods to save and retrieve any Codable type, though it's primarily used for storing AuthParameter objects containing access tokens.
TokenViewModel.swift manages the token lifecycle. On app startup, it fetches an access token using hardcoded client credentials, then stores the token securely in the keychain. The token includes consumer ID and account information needed for API calls.
NetworkService.swift handles the OAuth token exchange, making POST requests to Walmart's identity service to get access tokens using client credentials flow.
Product Search Pipeline
The search functionality connects to Walmart's product catalog API to find items.
ProductSearchService.swift makes authenticated API calls to search for products. It constructs URLs with search parameters and uses the stored authentication headers. The service returns StatusResponse objects containing arrays of matching products.
ProductSearchViewModel.swift provides the UI binding layer, managing search state (loading, results, errors) and coordinating between the search service and UI components.
The search results include product names, UPC numbers, descriptions, and image URLs that get displayed in the main interface.
Computer Vision Pipeline
This is the heart of the app - a sophisticated ML pipeline that processes camera frames to detect and identify products in real-time.
Object Detection (YOLO Model)
ProductEmbedder.swift is the main class handling ML inference. It loads two TensorFlow Lite models:

YOLOV8.tflite (87MB) - Object detection model that finds product bounding boxes in camera frames
mobilevit.tflite (2.2MB) - Feature extraction model that generates embeddings for detected objects

The object detection process:

Takes incoming camera frames (256x256 input size)
Preprocesses images by normalizing RGB values to [0,1] range
Runs inference through the YOLO model
Parses the 6x1344 output tensor to extract bounding box coordinates and confidence scores
Filters detections above a confidence threshold (0.75)
Applies Non-Maximum Suppression (NMS) to remove overlapping detections

Feature Embedding
For each detected object:

Crops the object region from the original frame using the bounding box coordinates
Resizes the crop to 128x128 while maintaining aspect ratio, padding with white pixels
Converts to BGR format and normalizes pixel values
Runs through the MobileViT embedding model
Generates a feature vector by averaging across spatial dimensions

Product Matching
EmbeddingMatcher.swift handles the matching of detected objects to known products:

Maintains a catalog of product embeddings fetched from the API
Uses cosine similarity to compare detected object embeddings with catalog embeddings
Implements an LRU cache to prioritize recently seen products for faster matching
Has configurable thresholds for match confidence and cache hits

EmbeddingsAPIService.swift fetches product embeddings from Walmart's API. It handles the complex response format where embeddings are base64-encoded and gzipped within the JSON response.
Frame Processing Pipeline
ShelfScanVM.swift orchestrates the entire computer vision pipeline:

Frame Capture - Receives frames from the camera manager through the delegate pattern
Serial Processing - Uses a dedicated queue to prevent overlapping inference operations
Detection - Runs object detection on every frame for responsive bounding boxes
Selective Embedding - Only runs embedding matching every Nth frame (configurable interval) to balance performance
Label Persistence - Caches product labels between frames using IoU (Intersection over Union) matching to reduce flicker
Concurrent Matching - When embedding matching runs, it processes all detected objects concurrently using DispatchGroup

Camera Integration
The camera system captures frames and delivers them to the ML pipeline.
CameraManager.swift uses ARKit's ARSession to capture camera frames. While ARKit is designed for augmented reality, here it's used simply as a robust camera interface that provides high-quality, properly oriented frames.
FrameManager.swift acts as a bridge, implementing a delegate pattern to deliver camera frames to the computer vision pipeline without coupling the camera system directly to the ML components.
ARViewContainer.swift provides the SwiftUI wrapper for the camera view, though the AR-specific functionality isn't actively used for product placement.
User Interface
The UI is built with SwiftUI and follows a navigation flow through several key screens.
LoginView.swift provides basic authentication with hardcoded credentials (admin/admin). This leads to store number entry.
StoreNumberView.swift validates the store number against a whitelist (100, 2119) before allowing access to the main app.
HomePageView.swift is the primary interface featuring:

A search bar with Walmart's blue branding
Product search results with images, names, and UPC codes
"Find item" buttons that trigger embedding download and camera activation

CameraView.swift presents the real-time detection interface:

Live camera feed background
Bounding box overlays showing detected products
Product information popup when tapping detected items
Blue header with close button and selected product name

BoundingBoxView.swift renders the detection overlays with configurable scale factors and offsets to align bounding boxes with the actual camera feed. It includes filtering logic to only show boxes matching the selected product ID.
Data Models
Product.swift defines the core data structures. Products contain UPC numbers, names, descriptions, image URLs, and some hardcoded fields like price and ratings since the API doesn't provide complete product information.
BoundingBox represents detected objects with rectangles, confidence scores, unique IDs, and optional product ID labels after matching.
Embedding pairs feature vectors with detection IDs to link ML outputs back to bounding boxes.
Performance Optimizations
The app includes several performance considerations:

Frame Skipping - Only runs expensive embedding matching periodically rather than every frame
Concurrent Processing - Uses TaskGroup to process multiple embeddings simultaneously
LRU Caching - Keeps recently identified products in fast memory for quicker re-identification
IoU Tracking - Reuses labels across frames when bounding boxes overlap significantly
Serial Queues - Prevents multiple inference operations from competing for model resources

Configuration & Environment
APIPaths.swift manages different API environments (staging, production, etc.) with corresponding base URLs and API versions. The current configuration points to staging environments.
The app uses hardcoded client credentials and consumer IDs for API authentication, which would typically be configured differently for production deployment.
Error Handling
The app includes error handling at multiple layers:

Network request failures with retry logic
ML model initialization errors
Authentication token expiration
Product search failures
Camera permission issues

Error states are surfaced through the UI with appropriate user messaging and recovery options.
This architecture provides a solid foundation for a computer vision-based product finding application, with clear separation of concerns and room for future enhancements like improved product matching algorithms or expanded product catalogs.
